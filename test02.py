import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib.parse
import re
from collections import Counter

# âœ… í‚¤ì›Œë“œ: ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´
SEARCH_KEYWORD = "ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´"

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”ì‹œì—ë§Œ)
kw_model = None
summarizer_available = False

def initialize_models():
    """í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ì´ˆê¸°í™”"""
    global kw_model, summarizer_available
    
    # KeyBERT ì´ˆê¸°í™” ì‹œë„
    try:
        from keybert import KeyBERT
        kw_model = KeyBERT()
        print("âœ… KeyBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        print("âš ï¸ KeyBERTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ì•ˆ í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        kw_model = None
    except Exception as e:
        print(f"âš ï¸ KeyBERT ë¡œë“œ ì˜¤ë¥˜: {e}")
        kw_model = None
    
    # gensim ìš”ì•½ ê¸°ëŠ¥ í™•ì¸
    try:
        from gensim.summarization import summarize
        summarizer_available = True
        print("âœ… gensim ìš”ì•½ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        print("âš ï¸ gensimì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ì•ˆ ìš”ì•½ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        summarizer_available = False
    except Exception as e:
        print(f"âš ï¸ gensim ë¡œë“œ ì˜¤ë¥˜: {e}")
        summarizer_available = False

# ëª¨ë¸ ì´ˆê¸°í™”
initialize_models()

# 1. ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ê°œì„ ëœ ë²„ì „)
def get_naver_news_articles(keyword, pages=1):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
    articles = []
    
    try:
        encoded_keyword = urllib.parse.quote(keyword)
        print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘: {keyword}")
        
        for page in range(1, pages + 1):
            start = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&start={start}"
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
            }
            
            print(f"ğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
            res = requests.get(url, headers=headers, timeout=10)
            res.raise_for_status()
            
            soup = BeautifulSoup(res.text, "html.parser")
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì…€ë ‰í„°
            news_items = soup.select(".news_area")
            
            if not news_items:
                print(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            page_articles = 0
            for item in news_items:
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_tag = item.select_one(".news_tit")
                    if not title_tag:
                        continue
                    
                    title = title_tag.get_text(strip=True)
                    link = title_tag.get("href", "")
                    
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if not title or not link or len(title) < 10:
                        continue
                    
                    # ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                    healthcare_keywords = ['ë””ì§€í„¸', 'í—¬ìŠ¤ì¼€ì–´', 'ì˜ë£Œ', 'ê±´ê°•', 'AI', 'ì¸ê³µì§€ëŠ¥', 'ì›ê²©ì§„ë£Œ', 'ì›¨ì–´ëŸ¬ë¸”', 'ìŠ¤ë§ˆíŠ¸', 'ë³‘ì›', 'IoT', 'ë¹…ë°ì´í„°']
                    if any(keyword in title for keyword in healthcare_keywords):
                        articles.append({
                            "title": title,
                            "link": link
                        })
                        page_articles += 1
                    
                except Exception as e:
                    print(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"ğŸ“° í˜ì´ì§€ {page}ì—ì„œ {page_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            time.sleep(1)  # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸°
        
        print(f"âœ… ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return articles
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return []

# 1-2. ì—°í•©ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ë°±ì—…ìš©)
def get_yna_article_links(keyword, pages=1):
    """ì—°í•©ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
    articles = []
    
    try:
        encoded_keyword = urllib.parse.quote(keyword)
        print(f"ğŸ” ì—°í•©ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘: {keyword}")
        
        # ì—°í•©ë‰´ìŠ¤ ê²€ìƒ‰ URL ìˆ˜ì •
        url = f"https://www.yna.co.kr/search?query={encoded_keyword}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
        }
        
        print(f"ğŸ“„ ì—°í•©ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ì—°í•©ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì…€ë ‰í„°ë“¤
        selectors = [
            ".list-type038 li",
            ".search-result-item",
            ".news-list li",
            "article"
        ]
        
        items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                print(f"âœ… ì…€ë ‰í„° '{selector}'ë¡œ {len(items)}ê°œ í•­ëª© ë°œê²¬")
                break
        
        if not items:
            print("âš ï¸ ì—°í•©ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        for item in items[:10]:  # ìµœëŒ€ 10ê°œ
            try:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_selectors = ["a.tit", ".tit a", "h3 a", "a"]
                title_tag = None
                
                for sel in title_selectors:
                    title_tag = item.select_one(sel)
                    if title_tag and title_tag.get("href"):
                        break
                
                if not title_tag:
                    continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag.get("href", "")
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if not title or not link or len(title) < 10:
                    continue
                
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if link.startswith("/"):
                    link = "https://www.yna.co.kr" + link
                elif not link.startswith("http"):
                    link = "https://www.yna.co.kr/" + link
                
                articles.append({
                    "title": title,
                    "link": link
                })
                
            except Exception as e:
                print(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        print(f"âœ… ì—°í•©ë‰´ìŠ¤ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return articles
        
    except Exception as e:
        print(f"âŒ ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return []

# 2. ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
def extract_article_text(url):
    """ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        print(f"ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {url[:50]}...")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì…€ë ‰í„°ë“¤
        content_selectors = [
            # ì—°í•©ë‰´ìŠ¤
            "div.story-news.article",
            "div.article-txt",
            ".article-wrap .article-txt",
            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            "div.news-content",
            "div.article-body",
            "div.article-content",
            ".news_end",
            ".article_body",
            "div.content",
            "article",
            ".post-content",
            ".entry-content",
            # ë„¤ì´ë²„ ë‰´ìŠ¤
            "#articleBodyContents",
            ".se-main-container"
        ]
        
        body_div = None
        for selector in content_selectors:
            body_div = soup.select_one(selector)
            if body_div:
                print(f"âœ… ë³¸ë¬¸ ë°œê²¬: {selector}")
                break
        
        if not body_div:
            # ëŒ€ì•ˆ: p íƒœê·¸ë“¤ ìˆ˜ì§‘
            paragraphs = soup.find_all('p')
            if paragraphs:
                # ê¸¸ì´ê°€ ì ë‹¹í•œ p íƒœê·¸ë“¤ë§Œ ì„ íƒ
                valid_paragraphs = [p for p in paragraphs if len(p.get_text(strip=True)) > 20]
                if valid_paragraphs:
                    text = ' '.join([p.get_text(strip=True) for p in valid_paragraphs])
                    print(f"âœ… p íƒœê·¸ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ({len(text)} ë¬¸ì)")
                    return text[:1500]  # ìµœëŒ€ 1500ì
            
            print("âš ï¸ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ''
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for unwanted in body_div.find_all(['script', 'style', 'iframe', 'ins', 'aside', 'nav', 'header', 'footer']):
            unwanted.decompose()
        
        text = body_div.get_text(separator=" ", strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text = ' '.join(text.split())  # ê³µë°± ì •ë¦¬
        text = text.replace('\n', ' ').replace('\t', ' ')
        
        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
        if len(text) < 50:
            print("âš ï¸ ì¶”ì¶œëœ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            return ''
        
        print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)} ë¬¸ì)")
        return text[:1500]  # ìµœëŒ€ 1500ìë¡œ ì œí•œ
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return ''
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ''

# 3. í…ìŠ¤íŠ¸ ìš”ì•½ (ê°œì„ ëœ ë²„ì „)
def summarize_text(text, ratio=0.3):
    """í…ìŠ¤íŠ¸ ìš”ì•½ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
    if not text or len(text.strip()) < 100:
        print("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return text[:200] if text else ''
    
    # ë°©ë²• 1: gensim ì‚¬ìš© (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if summarizer_available:
        try:
            from gensim.summarization import summarize
            summary = summarize(text, ratio=ratio)
            if summary:
                print(f"âœ… gensimìœ¼ë¡œ ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)")
                return summary
        except Exception as e:
            print(f"âš ï¸ gensim ìš”ì•½ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 2: sumy ì‚¬ìš© (ëŒ€ì•ˆ)
    try:
        from sumy.parsers.plaintext import PlaintextParser
        from sumy.nlp.tokenizers import Tokenizer
        from sumy.summarizers.text_rank import TextRankSummarizer
        
        parser = PlaintextParser.from_string(text, Tokenizer("korean"))
        summarizer = TextRankSummarizer()
        sentences = summarizer(parser.document, 3)  # 3ë¬¸ì¥ ìš”ì•½
        summary = ' '.join([str(sentence) for sentence in sentences])
        
        if summary:
            print(f"âœ… sumyë¡œ ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)")
            return summary
    except ImportError:
        print("âš ï¸ sumyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ sumy ìš”ì•½ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 3: ê°„ë‹¨í•œ ë¬¸ì¥ ì¶”ì¶œ (ë°±ì—…)
    try:
        sentences = text.split('.')
        # ê¸¸ì´ê°€ ì ë‹¹í•œ ë¬¸ì¥ë“¤ ì„ íƒ
        good_sentences = [s.strip() for s in sentences if 20 < len(s.strip()) < 200]
        
        if good_sentences:
            # ì²˜ìŒ 2-3ê°œ ë¬¸ì¥ ì„ íƒ
            summary = '. '.join(good_sentences[:3]) + '.'
            print(f"âœ… ê°„ë‹¨ ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)")
            return summary
        else:
            # ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ ë¶€ë¶„ ë°˜í™˜
            summary = text[:300] + '...' if len(text) > 300 else text
            print(f"âœ… í…ìŠ¤íŠ¸ ì¼ë¶€ ë°˜í™˜ ({len(summary)} ë¬¸ì)")
            return summary
            
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return text[:200] if text else ''

# 4. í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
def extract_keywords(text, top_n=5):
    """í‚¤ì›Œë“œ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
    if not text or len(text.strip()) < 50:
        print("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # ë°©ë²• 1: KeyBERT ì‚¬ìš© (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if kw_model:
        try:
            keywords = kw_model.extract_keywords(text, top_n=top_n, stop_words="english")
            result = [kw[0] for kw in keywords]
            print(f"âœ… KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {result}")
            return result
        except Exception as e:
            print(f"âš ï¸ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 2: konlpy ì‚¬ìš© (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„)
    try:
        from konlpy.tag import Okt
        okt = Okt()
        # í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
        nouns = okt.nouns(text)
        # ê¸¸ì´ê°€ 2 ì´ìƒì¸ ëª…ì‚¬ë§Œ ì„ íƒ
        filtered_nouns = [noun for noun in nouns if len(noun) >= 2]
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        noun_counts = Counter(filtered_nouns)
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        keywords = [word for word, count in noun_counts.most_common(top_n)]
        print(f"âœ… konlpyë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
        return keywords
    except ImportError:
        print("âš ï¸ konlpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ konlpy í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 3: ê°„ë‹¨í•œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ë°±ì—…)
    try:
        # í•œê¸€ ë‹¨ì–´ë§Œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        korean_words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ê²ƒì´', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ê°™ì€', 'ë§ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 
                    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ë•Œë¬¸', 'í†µí•´', 'ìœ„í•´',
                    'ëŒ€í•œ', 'ê´€ë ¨', 'ê²½ìš°', 'ë•Œë¬¸ì—', 'ì´í›„', 'ì´ì „', 'í˜„ì¬', 'ì˜¤ëŠ˜', 'ì–´ì œ',
                    'ê¸°ì', 'ë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„', 'ë°œí‘œ', 'ì„¤ëª…', 'ë§í–ˆë‹¤']
        
        filtered_words = [word for word in korean_words if word not in stopwords and len(word) >= 2]
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_counts = Counter(filtered_words)
        keywords = [word for word, count in word_counts.most_common(top_n)]
        
        print(f"âœ… ê°„ë‹¨ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
        return keywords
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

# 5. ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ (ê°œì„ ëœ ë²„ì „)
def run_pipeline():
    """ë©”ì¸ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸"""
    print("\n" + "="*60)
    print("ğŸ“° ì—°í•©ë‰´ìŠ¤ ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ê¸°ì‚¬ ë¶„ì„ ì‹œì‘")
    print("="*60)
    
    try:
        # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ (ë„¤ì´ë²„ ë‰´ìŠ¤ ìš°ì„  ì‹œë„)
        articles = get_naver_news_articles(SEARCH_KEYWORD, pages=1)
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨ì‹œ ì—°í•©ë‰´ìŠ¤ ì‹œë„
        if not articles:
            print("âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨, ì—°í•©ë‰´ìŠ¤ ì‹œë„...")
            articles = get_yna_article_links(SEARCH_KEYWORD, pages=1)
        
        if not articles:
            print("âŒ ì‹¤ì œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (7ê°œë¡œ í™•ì¥)
            print("ğŸ’¡ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            from datetime import datetime
            current_date = datetime.now().strftime('%Y-%m-%d')
            
            sample_data = [
                {
                    "title": "ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ì‹œì¥ ê¸‰ì„±ì¥, AI ê¸°ìˆ  ë„ì… í™•ì‚°",
                    "link": "https://example.com/news1",
                    "summary": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ í™œìš©í•œ ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ì‹œì¥ì´ ê¸‰ì†ë„ë¡œ ì„±ì¥í•˜ê³  ìˆë‹¤. ì›ê²© ì§„ë£Œì™€ ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤ ë“±ì´ ì£¼ìš” ì„±ì¥ ë™ë ¥ì´ ë˜ê³  ìˆë‹¤.",
                    "keywords": "ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´, ì¸ê³µì§€ëŠ¥, ì›ê²©ì§„ë£Œ, ì›¨ì–´ëŸ¬ë¸”",
                    "date": current_date
                },
                {
                    "title": "ì›ê²© ì§„ë£Œ í”Œë«í¼ í™•ì‚°ìœ¼ë¡œ ì˜ë£Œ ì ‘ê·¼ì„± í–¥ìƒ",
                    "link": "https://example.com/news2",
                    "summary": "ì½”ë¡œë‚˜19ë¥¼ ê³„ê¸°ë¡œ ì›ê²© ì§„ë£Œ ì„œë¹„ìŠ¤ê°€ ë³¸ê²©í™”ë˜ë©´ì„œ ì˜ë£Œ ì ‘ê·¼ì„±ì´ í¬ê²Œ í–¥ìƒë˜ê³  ìˆë‹¤.",
                    "keywords": "ì›ê²©ì§„ë£Œ, ì˜ë£Œì ‘ê·¼ì„±, ì½”ë¡œë‚˜19, í”Œë«í¼",
                    "date": current_date
                },
                {
                    "title": "ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤ë¡œ ê±´ê°• ê´€ë¦¬ í˜ì‹ ",
                    "link": "https://example.com/news3", 
                    "summary": "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ì™€ í”¼íŠ¸ë‹ˆìŠ¤ íŠ¸ë˜ì»¤ ë“±ì´ ê°œì¸ ê±´ê°• ê´€ë¦¬ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ê³  ìˆë‹¤.",
                    "keywords": "ì›¨ì–´ëŸ¬ë¸”, ìŠ¤ë§ˆíŠ¸ì›Œì¹˜, ê±´ê°•ê´€ë¦¬, í”¼íŠ¸ë‹ˆìŠ¤",
                    "date": current_date
                },
                {
                    "title": "ë¹…ë°ì´í„° í™œìš©í•œ ë§ì¶¤í˜• ì˜ë£Œ ì„œë¹„ìŠ¤ í™•ì‚°",
                    "link": "https://example.com/news4",
                    "summary": "ì˜ë£Œ ë¹…ë°ì´í„°ë¥¼ í™œìš©í•œ ê°œì¸ ë§ì¶¤í˜• ì¹˜ë£Œì™€ ì˜ˆë°© ì„œë¹„ìŠ¤ê°€ í™•ì‚°ë˜ê³  ìˆì–´ ì˜ë£Œ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”ë¥¼ ì´ëŒê³  ìˆë‹¤.",
                    "keywords": "ë¹…ë°ì´í„°, ë§ì¶¤í˜•ì˜ë£Œ, ì˜ˆë°©ì„œë¹„ìŠ¤, ì˜ë£ŒíŒ¨ëŸ¬ë‹¤ì„",
                    "date": current_date
                },
                {
                    "title": "IoT ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë³‘ì› ì‹œìŠ¤í…œ ë„ì… ê°€ì†í™”",
                    "link": "https://example.com/news5",
                    "summary": "ì‚¬ë¬¼ì¸í„°ë„· ê¸°ìˆ ì„ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ë³‘ì› ì‹œìŠ¤í…œì´ ë„ì…ë˜ë©´ì„œ í™˜ì ëª¨ë‹ˆí„°ë§ê³¼ ì˜ë£Œì§„ ì—…ë¬´ íš¨ìœ¨ì„±ì´ í¬ê²Œ í–¥ìƒë˜ê³  ìˆë‹¤.",
                    "keywords": "IoT, ìŠ¤ë§ˆíŠ¸ë³‘ì›, í™˜ìëª¨ë‹ˆí„°ë§, ì—…ë¬´íš¨ìœ¨ì„±",
                    "date": current_date
                },
                {
                    "title": "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ë¡œ ì˜ë£Œ ë°ì´í„° ë³´ì•ˆ ê°•í™”",
                    "link": "https://example.com/news6",
                    "summary": "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ ë„ì…í•˜ì—¬ ì˜ë£Œ ë°ì´í„°ì˜ ë³´ì•ˆì„±ê³¼ íˆ¬ëª…ì„±ì„ ë†’ì´ëŠ” ì‹œìŠ¤í…œì´ ê°œë°œë˜ì–´ ì£¼ëª©ë°›ê³  ìˆë‹¤.",
                    "keywords": "ë¸”ë¡ì²´ì¸, ì˜ë£Œë°ì´í„°, ë³´ì•ˆ, íˆ¬ëª…ì„±",
                    "date": current_date
                },
                {
                    "title": "VR/AR ê¸°ìˆ  í™œìš©í•œ ì˜ë£Œ êµìœ¡ ë° ì¹˜ë£Œ í˜ì‹ ",
                    "link": "https://example.com/news7",
                    "summary": "ê°€ìƒí˜„ì‹¤ê³¼ ì¦ê°•í˜„ì‹¤ ê¸°ìˆ ì´ ì˜ë£Œì§„ êµìœ¡ê³¼ í™˜ì ì¹˜ë£Œì— í™œìš©ë˜ë©´ì„œ ì˜ë£Œ ì„œë¹„ìŠ¤ì˜ ì§ˆì  í–¥ìƒì„ ê°€ì ¸ì˜¤ê³  ìˆë‹¤.",
                    "keywords": "VR, AR, ì˜ë£Œêµìœ¡, ì¹˜ë£Œí˜ì‹ ",
                    "date": current_date
                }
            ]
            
            sample_df = pd.DataFrame(sample_data)
            
            # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
            try:
                existing_df = pd.read_csv("digital_healthcare_news.csv")
                print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° {len(existing_df)}ê°œ ë°œê²¬")
                
                combined_df = pd.concat([existing_df, sample_df], ignore_index=True)
                combined_df = combined_df.drop_duplicates(subset=['title'], keep='last')
                
                # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
                if len(combined_df) > 10:
                    combined_df = combined_df.tail(10)
                    
            except FileNotFoundError:
                print("ğŸ“‚ ê¸°ì¡´ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                combined_df = sample_df
            
            # ë‘ íŒŒì¼ì— ëª¨ë‘ ì €ì¥
            combined_df.to_csv("yna_digital_healthcare_news.csv", index=False, encoding="utf-8-sig")
            combined_df.to_csv("digital_healthcare_news.csv", index=False, encoding="utf-8-sig")
            
            print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì´ {len(combined_df)}ê°œ ê¸°ì‚¬")
            print("ğŸ“ íŒŒì¼: yna_digital_healthcare_news.csv, digital_healthcare_news.csv")
            return
        
        results = []
        total_articles = min(len(articles), 7)  # ìµœëŒ€ 7ê°œ ê¸°ì‚¬ ì²˜ë¦¬
        
        print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤...\n")
        
        for i, article in enumerate(articles[:total_articles], 1):
            print(f"\n[{i}/{total_articles}] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
            print(f"ğŸ“° ì œëª©: {article['title'][:50]}...")
            
            try:
                # ë³¸ë¬¸ ì¶”ì¶œ
                full_text = extract_yna_article_text(article['link'])
                
                if not full_text:
                    print("âš ï¸ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ìš”ì•½ ìƒì„±
                print("ğŸ“ ìš”ì•½ ìƒì„± ì¤‘...")
                summary = summarize_text(full_text)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                print("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                keywords = extract_keywords(full_text)
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "title": article["title"],
                    "link": article["link"],
                    "text_length": len(full_text),
                    "summary": summary if summary else 'ìš”ì•½ ìƒì„± ì‹¤íŒ¨',
                    "keywords": ", ".join(keywords) if keywords else 'í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨',
                    "date": datetime.now().strftime('%Y-%m-%d')
                }
                
                results.append(result)
                print(f"âœ… ê¸°ì‚¬ {i} ì²˜ë¦¬ ì™„ë£Œ")
                
                # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸°
                if i < total_articles:
                    print("â³ 2ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(2)
                    
            except Exception as e:
                print(f"âŒ ê¸°ì‚¬ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²°ê³¼ ì €ì¥ ë° ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
        if results:
            try:
                new_df = pd.DataFrame(results)
                
                # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹œë„
                try:
                    existing_df = pd.read_csv("digital_healthcare_news.csv")
                    print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° {len(existing_df)}ê°œ ë°œê²¬")
                    
                    # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° í•©ì¹˜ê¸°
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    
                    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
                    combined_df = combined_df.drop_duplicates(subset=['title'], keep='last')
                    
                    # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
                    if len(combined_df) > 10:
                        combined_df = combined_df.tail(10)  # ìµœì‹  10ê°œ ìœ ì§€
                    
                    print(f"ğŸ“Š í•©ì³ì§„ ë°ì´í„°: {len(combined_df)}ê°œ")
                    
                except FileNotFoundError:
                    print("ğŸ“‚ ê¸°ì¡´ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    combined_df = new_df
                
                # ë‘ íŒŒì¼ì— ëª¨ë‘ ì €ì¥
                csv_filename = "yna_digital_healthcare_news.csv"
                combined_df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
                
                # app01.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” íŒŒì¼ì—ë„ ì €ì¥
                combined_df.to_csv("digital_healthcare_news.csv", index=False, encoding="utf-8-sig")
                
                print(f"\n" + "="*60)
                print("âœ… ë¶„ì„ ì™„ë£Œ!")
                print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {csv_filename}")
                print(f"ï¿½  app01.pyìš© íŒŒì¼: digital_healthcare_news.csv")
                print(f"ğŸ“Š ìƒˆë¡œ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(results)}ê°œ")
                print(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {len(combined_df)}ê°œ")
                print("="*60)
                
                # ì „ì²´ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                print("\nğŸ“‹ ì „ì²´ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
                for i, (_, row) in enumerate(combined_df.iterrows(), 1):
                    print(f"\n[{i}] {row['title'][:50]}...")
                    print(f"    ğŸ“ ìš”ì•½: {str(row['summary'])[:100]}...")
                    print(f"    ğŸ·ï¸ í‚¤ì›Œë“œ: {row['keywords']}")
                    if 'text_length' in row:
                        print(f"    ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {row['text_length']}ì")
                
            except Exception as e:
                print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
                
                # ë°±ì—…: í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                try:
                    with open("yna_digital_healthcare_news_backup.txt", "w", encoding="utf-8") as f:
                        f.write("ì—°í•©ë‰´ìŠ¤ ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ê¸°ì‚¬ ë¶„ì„ ê²°ê³¼\n")
                        f.write("="*50 + "\n\n")
                        
                        for i, result in enumerate(results, 1):
                            f.write(f"[{i}] {result['title']}\n")
                            f.write(f"ë§í¬: {result['link']}\n")
                            f.write(f"ìš”ì•½: {result['summary']}\n")
                            f.write(f"í‚¤ì›Œë“œ: {result['keywords']}\n")
                            f.write("-" * 50 + "\n\n")
                    
                    print("ğŸ’¾ ë°±ì—… íŒŒì¼ë¡œ ì €ì¥ë¨: yna_digital_healthcare_news_backup.txt")
                    
                except Exception as backup_error:
                    print(f"âŒ ë°±ì—… íŒŒì¼ ì €ì¥ë„ ì‹¤íŒ¨: {backup_error}")
        else:
            print("\nâŒ ì²˜ë¦¬ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        print("ğŸ” ìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    try:
        run_pipeline()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ”§ ë¬¸ì œê°€ ì§€ì†ë˜ë©´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("   pip install requests beautifulsoup4 pandas gensim keybert konlpy sumy")
    finally:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
